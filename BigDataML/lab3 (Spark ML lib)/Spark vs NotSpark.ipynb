{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with Spark and others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пра дадзеныя: невялікі датасэт, на 770 запісаў. Распавядае нам пра людзей, іх стан і тое, ці маюць яны дыябет.\\\n",
    "Там значна ніжэй, дзе пачынаецца NotSpark implementation, я зрабіў невялічкі аналіз выбаркі, яна не сбалансаваная, вось рэзультаты:\\\n",
    "% of people in safe:  65.10416666666667\\\n",
    "% of people in dangerous:  34.895833333333336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'pima-indians-diabetes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = spark.read.csv(file_name, header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+----+-----+---+-----+\n",
      "|_c0|_c1|_c2|_c3|_c4| _c5|  _c6|_c7|label|\n",
      "+---+---+---+---+---+----+-----+---+-----+\n",
      "|  6|148| 72| 35|  0|33.6|0.627| 50|    1|\n",
      "|  1| 85| 66| 29|  0|26.6|0.351| 31|    0|\n",
      "|  8|183| 64|  0|  0|23.3|0.672| 32|    1|\n",
      "+---+---+---+---+---+----+-----+---+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumnRenamed('_c8', 'label')\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_columns = data.columns[:-1]\n",
    "assembler = VectorAssembler(inputCols=feature_columns,outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+----+-----+---+-----+--------------------+\n",
      "|_c0|_c1|_c2|_c3|_c4| _c5|  _c6|_c7|label|            features|\n",
      "+---+---+---+---+---+----+-----+---+-----+--------------------+\n",
      "|  6|148| 72| 35|  0|33.6|0.627| 50|    1|[6.0,148.0,72.0,3...|\n",
      "|  1| 85| 66| 29|  0|26.6|0.351| 31|    0|[1.0,85.0,66.0,29...|\n",
      "|  8|183| 64|  0|  0|23.3|0.672| 32|    1|[8.0,183.0,64.0,0...|\n",
      "|  1| 89| 66| 23| 94|28.1|0.167| 21|    0|[1.0,89.0,66.0,23...|\n",
      "|  0|137| 40| 35|168|43.1|2.288| 33|    1|[0.0,137.0,40.0,3...|\n",
      "|  5|116| 74|  0|  0|25.6|0.201| 30|    0|[5.0,116.0,74.0,0...|\n",
      "|  3| 78| 50| 32| 88|31.0|0.248| 26|    1|[3.0,78.0,50.0,32...|\n",
      "| 10|115|  0|  0|  0|35.3|0.134| 29|    0|[10.0,115.0,0.0,0...|\n",
      "|  2|197| 70| 45|543|30.5|0.158| 53|    1|[2.0,197.0,70.0,4...|\n",
      "|  8|125| 96|  0|  0| 0.0|0.232| 54|    1|[8.0,125.0,96.0,0...|\n",
      "|  4|110| 92|  0|  0|37.6|0.191| 30|    0|[4.0,110.0,92.0,0...|\n",
      "| 10|168| 74|  0|  0|38.0|0.537| 34|    1|[10.0,168.0,74.0,...|\n",
      "| 10|139| 80|  0|  0|27.1|1.441| 57|    0|[10.0,139.0,80.0,...|\n",
      "|  1|189| 60| 23|846|30.1|0.398| 59|    1|[1.0,189.0,60.0,2...|\n",
      "|  5|166| 72| 19|175|25.8|0.587| 51|    1|[5.0,166.0,72.0,1...|\n",
      "|  7|100|  0|  0|  0|30.0|0.484| 32|    1|[7.0,100.0,0.0,0....|\n",
      "|  0|118| 84| 47|230|45.8|0.551| 31|    1|[0.0,118.0,84.0,4...|\n",
      "|  7|107| 74|  0|  0|29.6|0.254| 31|    1|[7.0,107.0,74.0,0...|\n",
      "|  1|103| 30| 38| 83|43.3|0.183| 33|    0|[1.0,103.0,30.0,3...|\n",
      "|  1|115| 70| 30| 96|34.6|0.529| 32|    1|[1.0,115.0,70.0,3...|\n",
      "+---+---+---+---+---+----+-----+---+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data_2.randomSplit([0.7, 0.3], seed=233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module pyspark.ml.classification:\n",
      "\n",
      "class RandomForestClassifier(pyspark.ml.wrapper.JavaEstimator, pyspark.ml.param.shared.HasFeaturesCol, pyspark.ml.param.shared.HasLabelCol, pyspark.ml.param.shared.HasPredictionCol, pyspark.ml.param.shared.HasSeed, pyspark.ml.param.shared.HasRawPredictionCol, pyspark.ml.param.shared.HasProbabilityCol, pyspark.ml.regression.RandomForestParams, TreeClassifierParams, pyspark.ml.param.shared.HasCheckpointInterval, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n",
      " |  RandomForestClassifier(featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity='gini', numTrees=20, featureSubsetStrategy='auto', seed=None, subsamplingRate=1.0)\n",
      " |  \n",
      " |  `Random Forest <http://en.wikipedia.org/wiki/Random_forest>`_\n",
      " |  learning algorithm for classification.\n",
      " |  It supports both binary and multiclass labels, as well as both continuous and categorical\n",
      " |  features.\n",
      " |  \n",
      " |  >>> import numpy\n",
      " |  >>> from numpy import allclose\n",
      " |  >>> from pyspark.ml.linalg import Vectors\n",
      " |  >>> from pyspark.ml.feature import StringIndexer\n",
      " |  >>> df = spark.createDataFrame([\n",
      " |  ...     (1.0, Vectors.dense(1.0)),\n",
      " |  ...     (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])\n",
      " |  >>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n",
      " |  >>> si_model = stringIndexer.fit(df)\n",
      " |  >>> td = si_model.transform(df)\n",
      " |  >>> rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=\"indexed\", seed=42)\n",
      " |  >>> model = rf.fit(td)\n",
      " |  >>> model.featureImportances\n",
      " |  SparseVector(1, {0: 1.0})\n",
      " |  >>> allclose(model.treeWeights, [1.0, 1.0, 1.0])\n",
      " |  True\n",
      " |  >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n",
      " |  >>> result = model.transform(test0).head()\n",
      " |  >>> result.prediction\n",
      " |  0.0\n",
      " |  >>> numpy.argmax(result.probability)\n",
      " |  0\n",
      " |  >>> numpy.argmax(result.rawPrediction)\n",
      " |  0\n",
      " |  >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n",
      " |  >>> model.transform(test1).head().prediction\n",
      " |  1.0\n",
      " |  >>> model.trees\n",
      " |  [DecisionTreeClassificationModel (uid=...) of depth..., DecisionTreeClassificationModel...]\n",
      " |  >>> rfc_path = temp_path + \"/rfc\"\n",
      " |  >>> rf.save(rfc_path)\n",
      " |  >>> rf2 = RandomForestClassifier.load(rfc_path)\n",
      " |  >>> rf2.getNumTrees()\n",
      " |  3\n",
      " |  >>> model_path = temp_path + \"/rfc_model\"\n",
      " |  >>> model.save(model_path)\n",
      " |  >>> model2 = RandomForestClassificationModel.load(model_path)\n",
      " |  >>> model.featureImportances == model2.featureImportances\n",
      " |  True\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      pyspark.ml.wrapper.JavaEstimator\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Estimator\n",
      " |      pyspark.ml.param.shared.HasFeaturesCol\n",
      " |      pyspark.ml.param.shared.HasLabelCol\n",
      " |      pyspark.ml.param.shared.HasPredictionCol\n",
      " |      pyspark.ml.param.shared.HasSeed\n",
      " |      pyspark.ml.param.shared.HasRawPredictionCol\n",
      " |      pyspark.ml.param.shared.HasProbabilityCol\n",
      " |      pyspark.ml.regression.RandomForestParams\n",
      " |      pyspark.ml.regression.TreeEnsembleParams\n",
      " |      pyspark.ml.param.shared.DecisionTreeParams\n",
      " |      TreeClassifierParams\n",
      " |      pyspark.ml.param.shared.HasCheckpointInterval\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity='gini', numTrees=20, featureSubsetStrategy='auto', seed=None, subsamplingRate=1.0)\n",
      " |      __init__(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\",                  maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                  maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity=\"gini\",                  numTrees=20, featureSubsetStrategy=\"auto\", seed=None, subsamplingRate=1.0)\n",
      " |  \n",
      " |  setFeatureSubsetStrategy(self, value)\n",
      " |      Sets the value of :py:attr:`featureSubsetStrategy`.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  setParams(self, featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, seed=None, impurity='gini', numTrees=20, featureSubsetStrategy='auto', subsamplingRate=1.0)\n",
      " |      setParams(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\",                   maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                   maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, seed=None,                   impurity=\"gini\", numTrees=20, featureSubsetStrategy=\"auto\", subsamplingRate=1.0)\n",
      " |      Sets params for linear classification.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __slotnames__ = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.wrapper.JavaEstimator:\n",
      " |  \n",
      " |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      " |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      " |      \n",
      " |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      " |      directly, and then acts as a mix-in class.  You can also register\n",
      " |      unrelated concrete classes (even built-in classes) and unrelated\n",
      " |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      " |      be considered subclasses of the registering ABC by the built-in\n",
      " |      issubclass() function, but the registering ABC won't show up in\n",
      " |      their MRO (Method Resolution Order) nor will method\n",
      " |      implementations defined by the registering ABC be callable (not\n",
      " |      even via super()).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  copy(self, extra=None)\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      :param extra: Extra parameters to copy to the new instance\n",
      " |      :return: Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Estimator:\n",
      " |  \n",
      " |  fit(self, dataset, params=None)\n",
      " |      Fits a model to the input dataset with optional parameters.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      " |      :param params: an optional param map that overrides embedded params. If a list/tuple of\n",
      " |                     param maps is given, this calls fit on each param map and returns a list of\n",
      " |                     models.\n",
      " |      :returns: fitted model(s)\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  fitMultiple(self, dataset, paramMaps)\n",
      " |      Fits a model to the input dataset for each param map in `paramMaps`.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`.\n",
      " |      :param paramMaps: A Sequence of param maps.\n",
      " |      :return: A thread safe iterable which contains one model for each param map. Each\n",
      " |               call to `next(modelIterator)` will return `(index, model)` where model was fit\n",
      " |               using `paramMaps[index]`. `index` values may not be sequential.\n",
      " |      \n",
      " |      .. note:: DeveloperApi\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  getFeaturesCol(self)\n",
      " |      Gets the value of featuresCol or its default value.\n",
      " |  \n",
      " |  setFeaturesCol(self, value)\n",
      " |      Sets the value of :py:attr:`featuresCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  getLabelCol(self)\n",
      " |      Gets the value of labelCol or its default value.\n",
      " |  \n",
      " |  setLabelCol(self, value)\n",
      " |      Sets the value of :py:attr:`labelCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  getPredictionCol(self)\n",
      " |      Gets the value of predictionCol or its default value.\n",
      " |  \n",
      " |  setPredictionCol(self, value)\n",
      " |      Sets the value of :py:attr:`predictionCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n",
      " |  \n",
      " |  getSeed(self)\n",
      " |      Gets the value of seed or its default value.\n",
      " |  \n",
      " |  setSeed(self, value)\n",
      " |      Sets the value of :py:attr:`seed`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n",
      " |  \n",
      " |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n",
      " |  \n",
      " |  getRawPredictionCol(self)\n",
      " |      Gets the value of rawPredictionCol or its default value.\n",
      " |  \n",
      " |  setRawPredictionCol(self, value)\n",
      " |      Sets the value of :py:attr:`rawPredictionCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n",
      " |  \n",
      " |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n",
      " |  \n",
      " |  getProbabilityCol(self)\n",
      " |      Gets the value of probabilityCol or its default value.\n",
      " |  \n",
      " |  setProbabilityCol(self, value)\n",
      " |      Sets the value of :py:attr:`probabilityCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n",
      " |  \n",
      " |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.regression.RandomForestParams:\n",
      " |  \n",
      " |  getNumTrees(self)\n",
      " |      Gets the value of numTrees or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setNumTrees(self, value)\n",
      " |      Sets the value of :py:attr:`numTrees`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.regression.RandomForestParams:\n",
      " |  \n",
      " |  numTrees = Param(parent='undefined', name='numTrees', doc='Number of t...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.regression.TreeEnsembleParams:\n",
      " |  \n",
      " |  getFeatureSubsetStrategy(self)\n",
      " |      Gets the value of featureSubsetStrategy or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  getSubsamplingRate(self)\n",
      " |      Gets the value of subsamplingRate or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setSubsamplingRate(self, value)\n",
      " |      Sets the value of :py:attr:`subsamplingRate`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.regression.TreeEnsembleParams:\n",
      " |  \n",
      " |  featureSubsetStrategy = Param(parent='undefined', name='featureSubsetS...\n",
      " |  \n",
      " |  subsamplingRate = Param(parent='undefined', name='subsamplingRate'...r...\n",
      " |  \n",
      " |  supportedFeatureSubsetStrategies = ['auto', 'all', 'onethird', 'sqrt',...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.DecisionTreeParams:\n",
      " |  \n",
      " |  getCacheNodeIds(self)\n",
      " |      Gets the value of cacheNodeIds or its default value.\n",
      " |  \n",
      " |  getMaxBins(self)\n",
      " |      Gets the value of maxBins or its default value.\n",
      " |  \n",
      " |  getMaxDepth(self)\n",
      " |      Gets the value of maxDepth or its default value.\n",
      " |  \n",
      " |  getMaxMemoryInMB(self)\n",
      " |      Gets the value of maxMemoryInMB or its default value.\n",
      " |  \n",
      " |  getMinInfoGain(self)\n",
      " |      Gets the value of minInfoGain or its default value.\n",
      " |  \n",
      " |  getMinInstancesPerNode(self)\n",
      " |      Gets the value of minInstancesPerNode or its default value.\n",
      " |  \n",
      " |  setCacheNodeIds(self, value)\n",
      " |      Sets the value of :py:attr:`cacheNodeIds`.\n",
      " |  \n",
      " |  setMaxBins(self, value)\n",
      " |      Sets the value of :py:attr:`maxBins`.\n",
      " |  \n",
      " |  setMaxDepth(self, value)\n",
      " |      Sets the value of :py:attr:`maxDepth`.\n",
      " |  \n",
      " |  setMaxMemoryInMB(self, value)\n",
      " |      Sets the value of :py:attr:`maxMemoryInMB`.\n",
      " |  \n",
      " |  setMinInfoGain(self, value)\n",
      " |      Sets the value of :py:attr:`minInfoGain`.\n",
      " |  \n",
      " |  setMinInstancesPerNode(self, value)\n",
      " |      Sets the value of :py:attr:`minInstancesPerNode`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.DecisionTreeParams:\n",
      " |  \n",
      " |  cacheNodeIds = Param(parent='undefined', name='cacheNodeIds', d...ed o...\n",
      " |  \n",
      " |  maxBins = Param(parent='undefined', name='maxBins', doc='M...mber of c...\n",
      " |  \n",
      " |  maxDepth = Param(parent='undefined', name='maxDepth', doc='...; depth ...\n",
      " |  \n",
      " |  maxMemoryInMB = Param(parent='undefined', name='maxMemoryInMB', ...ati...\n",
      " |  \n",
      " |  minInfoGain = Param(parent='undefined', name='minInfoGain', do...in fo...\n",
      " |  \n",
      " |  minInstancesPerNode = Param(parent='undefined', name='minInstancesPerN...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from TreeClassifierParams:\n",
      " |  \n",
      " |  getImpurity(self)\n",
      " |      Gets the value of impurity or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setImpurity(self, value)\n",
      " |      Sets the value of :py:attr:`impurity`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from TreeClassifierParams:\n",
      " |  \n",
      " |  impurity = Param(parent='undefined', name='impurity', doc='...-insensi...\n",
      " |  \n",
      " |  supportedImpurities = ['entropy', 'gini']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n",
      " |  \n",
      " |  getCheckpointInterval(self)\n",
      " |      Gets the value of checkpointInterval or its default value.\n",
      " |  \n",
      " |  setCheckpointInterval(self, value)\n",
      " |      Sets the value of :py:attr:`checkpointInterval`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n",
      " |  \n",
      " |  checkpointInterval = Param(parent='undefined', name='checkpointInterv....\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param)\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self)\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra=None)\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      :param extra: extra param values\n",
      " |      :return: merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param)\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName)\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param)\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName)\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param)\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param)\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param, value)\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self)\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path)\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() from builtins.type\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path) from builtins.type\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol='label', featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут, канешне, так сабе пайплайн, усяго з адной ступенню, але які ёсць"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 199 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"label\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "standard_accuracy_roc = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8079582474569517"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_accuracy_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7044998642442541"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.setMetricName('areaUnderPR')\n",
    "standard_accuracy_pr = evaluator.evaluate(predictions)\n",
    "standard_accuracy_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Чамусьці ў BinaryClassificationEvaluator ё толькі дзве гэтыя метрыкі, больш няма. Другая щвогуле малараспаўсюджаная.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid through parameters in RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol='label', featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [100, 200, 300]).addGrid(rf.maxBins, [32, 64, 128]).addGrid(rf.maxDepth,[5, 10, 20]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=5) \n",
    "\n",
    "cvModel = crossval.fit(data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Воьс прыблізна столькі на гэтым датасэце і маім слабым камп'ютары заняў пошук лепшых параметраў, з 27 рознымі канфігурацыямі і 5-фолд валідацыяй."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.tuning.CrossValidatorModel'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8347954270314932,\n",
       " 0.8283731993323638,\n",
       " 0.8255025134592198,\n",
       " 0.8419463350535121,\n",
       " 0.8336457287398729,\n",
       " 0.8327314297313286,\n",
       " 0.8425275770007015,\n",
       " 0.8346027758157477,\n",
       " 0.8314479359338092,\n",
       " 0.8369377951281737,\n",
       " 0.8368779858058869,\n",
       " 0.837337728588565,\n",
       " 0.8406953351694999,\n",
       " 0.8319710937455133,\n",
       " 0.8308763362918935,\n",
       " 0.8395962377266448,\n",
       " 0.8368078932683352,\n",
       " 0.8328624406691981,\n",
       " 0.8422427878926466,\n",
       " 0.8366830161418526,\n",
       " 0.834232616732181,\n",
       " 0.8428661727740268,\n",
       " 0.8414521529817844,\n",
       " 0.8398795522979523,\n",
       " 0.8413511509008278,\n",
       " 0.8333847546472761,\n",
       " 0.8333665812981119]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(cvModel))\n",
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Паглядзім на канфігурацыю лепшай мадэлі:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth =  5\n",
      "Num of Trees =  100\n",
      "Max Bins =  64\n"
     ]
    }
   ],
   "source": [
    "max_depth = cvModel.bestModel.stages[-1]._java_obj.getMaxDepth()\n",
    "print(\"Max Depth = \", max_depth)\n",
    "num_trees = cvModel.bestModel.stages[-1]._java_obj.getNumTrees()\n",
    "print(\"Num of Trees = \", num_trees)\n",
    "max_bins = cvModel.bestModel.stages[-1]._java_obj.getMaxBins()\n",
    "print(\"Max Bins = \", max_bins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Пабудуем мадэль зноў, каб навучыць яе на трэйне і праверыць на тэсце, бо інакш параўнанне дакладнасці двух мадэляў не будзе шчырым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol='label', featuresCol=\"features\", maxBins=max_bins, maxDepth=max_depth, numTrees=num_trees)\n",
    "pipeline = Pipeline(stages=[rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: areaUnderROC\n",
      "Standard configuration on test: 0.8079582474569517\n",
      "Best parameters by griding: 0.8218203576889841\n",
      "\n",
      "Metric: areaUnderPR\n",
      "Standard configuration on test: 0.7044998642442541\n",
      "Best parameters by griding: 0.7109505838078012\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.setMetricName('areaUnderROC')\n",
    "grid_accuracy_roc = evaluator.evaluate(predictions)\n",
    "print(\"Metric: areaUnderROC\")\n",
    "print(f\"Standard configuration on test: {standard_accuracy_roc}\")\n",
    "print(f\"Best parameters by griding: {grid_accuracy_roc}\")\n",
    "\n",
    "evaluator.setMetricName('areaUnderPR')\n",
    "grid_accuracy_pr = evaluator.evaluate(predictions)\n",
    "print(\"\\nMetric: areaUnderPR\")\n",
    "print(f\"Standard configuration on test: {standard_accuracy_pr}\")\n",
    "print(f\"Best parameters by griding: {grid_accuracy_pr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Змаглі крыху палепшыць рэзультат, але не моцна.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NotSpark implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RandForestClf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_name, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8], dtype='int64')"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3  4     5      6   7  8\n",
       "0  6  148  72  35  0  33.6  0.627  50  1\n",
       "1  1   85  66  29  0  26.6  0.351  31  0\n",
       "2  8  183  64   0  0  23.3  0.672  32  1"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут можна паглядзець на тое, як у нас дадзеныя размеркаваныя ў датасэце.\n",
    "Гэта варта было зрабіць у самым пачатку, але я быў заняты тым, што прадзіраўся праз спарк, таму лепей позна, чым ніколі. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(figsize=(6,4))\n",
    "sns.countplot(x=8, data=data)\n",
    "plt.title(\"Ці будзе дыябет у чалавека\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of people in safe:  65.10416666666667\n",
      "% of people in dangerous:  34.895833333333336\n"
     ]
    }
   ],
   "source": [
    "n=len(data)\n",
    "l_0=len(data[data[8]==0])\n",
    "l_1=len(data[data[8]==1])\n",
    "\n",
    "print(\"% of people in safe: \",surv_0*100/n)\n",
    "print(\"% of people in dangerous: \",surv_1*100/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[data.columns[:-1]]\n",
    "y = data[data.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Random Forest from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandForestClf(random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "random_forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred=random_forest.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7445887445887446\n",
      "ROC:  0.6909250879839115\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"ROC: \", sklearn.metrics.roc_auc_score(y_test, y_pred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth' : [4, 5, 6, 10, 20],\n",
    "    'criterion' :['gini', 'entropy'],\n",
    "    'max_features' :['auto', 'sqrt'],\n",
    "    'min_samples_split' :[2, 5, 10],\n",
    "    'min_samples_leaf' :[1, 2, 4],\n",
    "    'bootstrap' :[True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1080 канфігурацый, ну пагналі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 54min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,...\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'bootstrap': [True, False],\n",
       "                         'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [4, 5, 6, 10, 20],\n",
       "                         'max_features': ['auto', 'sqrt'],\n",
       "                         'min_samples_leaf': [1, 2, 4],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [100, 200, 300]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rand_forest_grid = RandForestClf(random_state=12)\n",
    "cv_rfc = GridSearchCV(estimator=rand_forest_grid, param_grid=param_grid, cv= 5)\n",
    "cv_rfc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': 20,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_best_rf = RandForestClf(random_state=12, criterion='entropy', max_depth=20, n_estimators=100, max_features='auto', min_samples_split=5, min_samples_leaf=2,bootstrap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=20, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=2, min_samples_split=5,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=12, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_best_rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = grid_best_rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest:  0.7835497835497836\n",
      "ROC:  0.7360482654600302\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Random Forest: \", sklearn.metrics.accuracy_score(y_test,predictions))\n",
    "print(\"ROC: \", sklearn.metrics.roc_auc_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вынікі і думкі\n",
    "Sklearn на грыдзе працаваў хутчэй за спарк, таму я дадаў больш параметраў для перабору, у выніку тут перабор займае болей часу. Гэта вельмі прыблізна, але:\\\n",
    "    спарк: 27 канфігурацый ~ 10 хвілін -> 2,7 канфігурацыі/хвіліну\\\n",
    "    sklearn: 1080 канфігурацый ~ 54 хвіліны -> 20 канфігурацый/хвіліну\n",
    "\n",
    "Пэўна, спарк вельмі хуткі на кластэрах, але для запуску лакальна лепей карыстацца іншымі мл-лібамі.\n",
    "\n",
    "Дзіўна крыху, што калі параўноўваць метрыкі, то ў спарку ROC-метрыка вышэй за склёрн на амаль на 10% (0.82 > 0.74). Прычым на спарку мы перабралі мала параметраў, у адрозненні ад склёрн. Можа, там розныя алгарытмы падліку ці так няўдачна атрымалася разбіць датасэт у склёрне. Не ведаю.\\\n",
    "Таксама мяне здзівіла, што для ў ацэншчыка бінарнай класіфікацыі няма звычайнай accuracy-метрыкі. Хаця ў нас датасэт не сбалансаваны, таму нам не вельмі яна патрэбная, гэтая аккурасі, канешне.\\\n",
    "Дзіўна, тут нейкая анамальная перамога спарка. Лес у склёрне так дрэнна рэалізаваны ці гэта мае рукі не вельмі рэалізаваныя, я не магу сказаць.\n",
    "\n",
    "Асалоды больш атрымаў не на спарку, бо штохвілінныя памылкі з хвастамі джава-экспэпшынаў на 40 радкоў моцна бянтэжаць. Плюс усё не зусім звычайна, не ведаеш, чым карыстацца, а дакументацыя спарка не самая лепшая, як мне падалося. Ну і вы паглядзіце, як я даставаў лепшыя параметры пры грыдзе ў спарку, гэта ж дзіч. \n",
    "Але, напэўна, гэта справа звычкі. Калі некалькі разоў папрацуеш, будзе лепей."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
